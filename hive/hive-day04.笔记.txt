avro + pb
-------------
	串行化技术。
	java
	writable
	1.avro
		使用json shema(数据结构)描述。
		紧凑。
	2.google protocal buffer
		跨语言，
		size很小。
		字段编号。
	
	3.
		java avro pb 


并行和并发
---------------
	1)并行
		并行计算,多台主机使用集群模式同时进行计算，典型的分布式计算。
		概念是粒度是粗的，宏观的概念。

	2)并发
		并发执行，通常是一个进程内同时执行的线程。
		粒度是比较细的。
		高并发，高负载，高吞吐量.

hive优化
---------------
	1.job和查询优化
		1.1)local mode
			和hadoop的集群模式，数据量小，可以使用hadoop的本地模式执行。
			SET hive.exec.mode.local.auto=true;						--default false
			SET hive.exec.mode.local.auto.inputbytes.max=50000000;	--输入字节数,默认128M
			SET hive.exec.mode.local.auto.input.files.max=5;		--输入文件数，默认4.

	2.JVM重用
		主要是task是轻量的，可以重复使用jvm，避免启动新的jvm浪费时间。
		该属性在hadoop第一代mr框架有效，不是yarn。
		SET mapred.job.reuse.jvm.num.tasks=5;
	
	3.并行执行
		hive的查询过程翻译出来的stage之间如果不存在依赖关系，该属性就生效。
		SET hive.exec.parallel=true;								--default false
		SET hive.exec.parallel.thread.number=16;					--可以并行执行的最大线程数，默认8
		

	4.join优化
		common连接指的是reduce连接。可通过如下的暗示语句实现:
		/*+ STREAMTABLE(stream_table_name) */.
		select /*+ STREAMTABLE(custs) */a.*,b.* from orders a left outer join custs b on a.cid = b.id ;
		4.1)map端连接
			SET hive.auto.convert.join=true;								--启用自动转换map端连接，default false
			SET hive.mapjoin.smalltable.filesize=600000000;					--小表的size,默认25m
			--????????  ???????
			SET hive.auto.convert.join.noconditionaltask=true;				--
			SET hive.auto.convert.join.noconditionaltask.size=10000000;		--

	5.Bucket map join
		桶表连接优化，所有表都是桶表。大表桶数是小表桶数的整倍数。
		SET hive.auto.convert.join=true;									--default false
		SET hive.optimize.bucketmapjoin=true;								--default false

	6.skew join(倾斜链接)
		如果数据分布不均匀，导致数据倾斜。
		SET hive.optimize.skewjoin=true;									--默认false
		SET hive.skewjoin.key=100000;										--判定是否倾斜的标准,新key不再发送给该reduce，
																			--而是发送给未使用的reduce
		重点--分组数据倾斜优化，单独启动一个mr随机分发key.
		SET hive.groupby.skewindata=true;

hive不使用order by实现全排序
----------------------------
	1.说明
		order by使用一个reduce,将所有数据发送一个节点进行排序。
		对内存要求很大。

	2.使用distribute by + Sort by实现全排序。
		思路
		自行设置reduce的个数4.
		set mapreduce.job.reduces=5
		手动对记录数据进行划分，1925|1950|1975
		在每个reduce内按照对year升序，temp降序排列。

	3.操作。
		3.1)创建表
			create table temps(year int,temp int) row format delimited fields terminated by ' ' lines terminated by '\n' stored as textfile ;

		3.2)加载数据
			hdfs dfs -put temp.dat hdfs://s101/user/hive/warehouse/mydb.db/temps/
		
		3.3)设置reduce个数
			select year,temp from temps distribute by case when year <1925 then 0 when year > 1975 then 3 when year > 1925 and year < 1950 then 1 else 2 end sort by year asc , temp desc ;

动态分区
----------------------------
	1.创建表
		create table part2(id int , name string , age int) partitioned by (hour int , minu int , sec int) row format delimited fields terminated by ',' lines terminated by '\n' stored as textfile ;
	2.手动添加分区
		alter table part2 add partition(hour=12,minu=12,sec=12) ;
	3.加载数据到分区表
		--静态分区
		load data local inpath '/home/centos/custs.txt' into table part2 partition(hour=12,minu=12,sec=12);

		--修改动态分区的严格模式,严格模式下，动态分区至少要指定一个静态分区列.
		set hive.exec.dynamic.partition.mode=nonstrict

		--动态插入所有分区字段
		insert into part2 partition(hour,minu,sec) select 6, 'tom6',6, date_format(current_timestamp(),'HH') ,date_format(current_timestamp(),'mm') ,date_format(current_timestamp(),'ss')  ;
	4.

自定义函数
---------------------------
	1.UDF
		user define function .
		自定义函数.
		给参数计算出一个结果。
		1.1)实现UDF类
			package com.it18zhang.hive.udf;

			import org.apache.hadoop.hive.ql.exec.Description;
			import org.apache.hadoop.hive.ql.exec.UDF;

			/**
			 * 自定义加法UDF
			 */
			@Description(name = "add", value = "add(1,2)", extended = "this is a extended info for add func!!")
			public class AddUDF extends UDF {

				public int evaluate(int a, int b) {
					return a + b;
				}
			}

		1.2)导出jar包
			...

		1.3)部署jar包hive所在的centos主机。
			...
		1.4)在hive的命令下添加jar包。
			打开hive cli后，导入的新的jar需要手动add jar一次，目的是添加jar到hive的类路径。
			如果是重启hive，只要jar位于hive/lib下即可。
			$hive>add jar /home/centos/downloads/my-hive.jar ;
		
		1.5)注册函数
			注册到当前的数据库中。
			如果没有指定数据库，是默认库(default)
			--临时函数,不会在mysql中生成记录,每次都要重新注册。
			$hive>create temporary function add AS 'com.it18zhang.hive.udf.AddUDF';

			--永久函数,会在mysql中生成注册记录，不需要每次重新注册。
			$hive>create function add AS 'com.it18zhang.hive.udf.AddUDF';

		1.6)调用函数
			hive>select default.add(1,2) ;
		
		1.7)注意查看mysql的信息。
			select * from bigg_hive.funcs ;
			select * from bigg_hive.func_ru ;
	2.UDAF
		user define aggreage function 
		自定义聚合函数。
		操纵多条记录，产生一个结果。

	3.UDTF
		user define table function 
		自定义表生成函数。
		explode()
		latern view 
		单行作为输入,输出多行或者多列。
		

练习
---------------
	1.练习一
		统计历年的最高气温，最低气温，平均气温(保留两位小数)，并按照year全排序，统计结果按年度时段保存5个文件。
		不使用order by。

		set mapreduce.job.reduces=5 ;

		create table res14 row format delimited fields terminated by ',' lines terminated by '\n' stored as textfile as select year,max(temp),min(temp),round(avg(temp),2) from temps group by year distribute by if(year < 1920 , 0 , if(year < 1940 ,1,if(year < 1960 , 2 , if(year < 1980 , 3 , 4)))) sort by year asc ;


	2.练习二
		2.1)自定义udf，将标签生成案例中的商家评论数据，使用函数提取出来。
			77287793	{"reviewPics":null,"extInfoList":[{"title":"contentTags","values":["音响效果好","体验好","价格实惠"],"desc":"","defineType":0},{"title":"tagIds","values":["173","499","373"],"desc":"","defineType":0}],"expenseList":null,"reviewIndexes":[2],"scoreList":null}

			1、创建hive表。
				
			2、使用fastJson提取出来values
				public void testRead(){
					//解析json文本成JSONObject
					JSONObject jo = JSON.parseObject() ;
					if(jo != null){
						JSONArray arr = jo.getJSONArray("extInfoList");
						if(arr != null && arr.size() > 0){
							JSONObject o1 = arr.getJSONObject(0) ;
							if(o1 != null){
								JSONArray arr2 = o1.getJSONArray("values") ;
								if(arr2 != null && arr2.size() > 0){
									for(int i = 0 ; i < arr2.size() ; i ++){
										String tag = arr2.getString(i);
										System.out.println(tag);
									}
								}
							}
						}
					}
				}

			3、

						
			提示，udf中使用fastjson库解析json文件，返回评论字符串！

		2.2)使用hive统计商家评论的倒排序。
