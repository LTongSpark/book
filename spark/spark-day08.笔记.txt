spark streaming
-----------------
	流计算。
	数据不断产生的。
	core
	sql
	spark streaming
	mllib
	graph

	准实时计算，batch(小批量计算)
	DStream			//离散流,连续的RDD。


	1.batch
	2.window
		窗口化操作，在batch基础上扩展。
		将若干batch形成的集合成为window长度，会有batch重复计算的情况。
		滑动间隔是batch的整倍数，滑动间隔是值计算的频率。
		xxxByKey()
		xxxByKeyAndWindow(func, win length , slide interval) ;

		DStream.window(winlen,slide interval);

	3.updateStateByKey
		按照key更新状态，key的状态。状态可以是任意类型。
		从应用启动开始，所有的key都一直保留下来的。
		更新函数func的返回值是Option[T],如果返回None，就丢弃key。

	4.spark streaming + sql
		val spark = new SparkSession().build().conf().getOrCreate();
		import spark.streaming._
		val df = rdd.toDF()
		df.createOrReplaceTempView("_temp")
		spark.sql("...")

	5.ssc.textSocketStream()
		ReceiverInputDStream() ->compute() --> BlockRDD

	6.Streaming + kafka集成
		*****kafka下的每个每个主题分区对应于一个RDD的分区。*****
		streaming_kafka_0.10,
		KafkaUtil.createDirectStream() 
			-> DirectKafkaInputDStream.compute() 
				-> KafkaRDD.getPartitions() 
					->  offsetRanges.zipWithIndex.map { 
							case (o, i) => new KafkaRDDPartition(i, o.topic, o.partition, o.fromOffset, o.untilOffset)
						}.toArray

		
		//对应关系,kafka的一个主题分区对应一个或多个RDD的分区。
		//RDD的分区是有OffsetRange(topic,parttion,startOffset, untilOffset)
		DirectKafkaInputDStream -> KafkaRDD -> getpartitions() -> KafkaRDDPartition(i, o.topic, o.partition, o.fromOffset, o.untilOffset)



	7.检查点
		如果容错，需要存足够的信息进行故障恢复,有两种类型数据需要检查:
		1.元数据
			定义了流计算的信息，可用于运行driver节点的主机上进行故障恢复。
			元数据包含:
			a)配置信息
				用来创建流应用的。
			b)DStream操作
				定义了流应用的DStream操作集合。
			c)未完成的batch
				在queue中但还未完成的batch.

		2.数据检查点
			将生成的RDD存储到可靠设备(hdfs),中间rdd的结果存储起来。
		

		3.启用检查点时机
			1.有状态变换
				updateStateByKey or reduceByKeyAndWindow 
			2.Driver的故障恢复
		
		4.如何配置检查点
			检查点就是目录存放检查信息，通过 streamingContext.checkpoint(checkpointDirectory)设置。
			如果对driver容错，需要保证两点:
			4.1)首次运行driver，创建新的ssc
			4.2)如果driver是容错重启的，需要从检查点目录恢复之前的driver数据.
		
		5.检查点程序
			import org.apache.spark.SparkConf
			import org.apache.spark.streaming.{Seconds, StreamingContext}

			/**
			  * Created by Administrator on 2018/3/8.
			  */
			object SparkStreamingWordCountScala2 {
				def main(args: Array[String]): Unit = {
					val conf = new SparkConf()
					conf.setAppName("worldCount")
					conf.setMaster("local[2]")

					//时间片是2秒
					def createSSC()={
						val ssc = new StreamingContext(conf, Seconds(2)) // new context
						//检查点目录设置
						ssc.checkpoint("file:///d:/java/chk")

						/************************************************************
						 ************* 注意：DStream需要设置checkPoint时间  *********
						 ************************************************************/
						ssc.socketTextStream("localhost", 8888).checkpoint(Seconds(2)).flatMap(_.split(" ")).map((_, 1)).window(Seconds(3600),Seconds(2)).print()
						//val result = ssc.socketTextStream("localhost", 8888).flatMap(_.split(" ")).map((_, 1))


						def updateFunc(a:Seq[Int],b:Option[Int]):Option[Int] = {
							var count:Int = 0 ;
							for(e <- a){
								count = count + e
							}

							var newcount:Int = 0 ;
							if(b.isEmpty){
								newcount = count ;
							}
							else{
								newcount = count + b.get;
							}

							if(newcount >= 5){
								None
							}
							else{
								Some(newcount)
							}
						}
						//result.updateStateByKey(updateFunc).print()

						ssc
					}

					val ssc = StreamingContext.getOrCreate("file:///d:/java/chk", createSSC _)

					//启动流
					ssc.start()

					ssc.awaitTermination()
				}
			}

spark Streaming调优
-----------------------
	1.为了高效利用集群，减少每个batch的处理时间
		
	2.尽可能快处理掉，争取的接收数据相同。

	3.receiver的并发度控制
		???比如kafka一个离散流接受两个主题分区数据，可以改成两个离散流，每个接受一个主题分区。

		spark.streaming.blockInterval

		task个数 = (batch interval / block interval)，如果task个数少于cores，说明有core处于空闲态。
		
		适当增加rdd形成的task个数，减少block interval(spark.streaming.blockInterval=200ms)值。推荐最小block interval = 50ms
		单流改成多流提高并发度的代替方案是再分区。

		说明：
		receiver接受数据后，推送到spark内存之前，先合并成block，合并成block时间间隔由
		spark.streaming.blockInterval=200ms控制，适当减少该值，达到增加task数的目的，就
		可以尽可能利用集群的资源。

	4.数据处理并发度
		spark.default.parallelism控制并发度,控制分区数。
		Memory_only > Memory_Ser -> Memory_+disk ->memor+disk 

传统统计分析手段
-------------------
	采样机制，导致不能正确全部的数据结果。
	大数据的分析师全样本分析，每条数据都参与计算。


spark mllib
-------------------
	1.机器学习
		通过数据 -> 训练模型(函数) -> 测试模型 -> 应用模型 -> 得出结果。
		用于预测。

	2.基础
		2.1)kmean
			均值
		2.2)median
			中位数
		2.3)mode
			众数

		2.4)range
			max - min

		2.5)variance
			方差。
			(x1 - xn)^2 + (x2 - xn)^2 + ...
			-------------------------------
						n
		2.6)standard deviation
			
            2 /------------
			\/   方差
		
		2.7)skewness
			偏度
			数据在均值两侧的偏差程度。
		
		2.8)kertosis
			峰度。
			曲线平滑或凸起的程度。
		
		
机器学习的类别
---------------
	1.监督学习
		有训练数据的(打了标签的)。
		算法:
			神经网络
			SVM(支持向量机)
			朴素贝叶斯

			分类技术。


	2.非监督学习
		使用未标签化数据集。
		kmean
		聚类


	3.推荐
		


朴素贝叶斯
---------------
	1.P(A)
		A事件发生的概率

	2.P(B|A)
		A事件已经发生，B事件发生的概率。

					P(A|B) * P(B)
		P(B|A) = ------------------
						P(A)
		
		事件A : 取出的红球

		事件B : 球来自于1号容器
		
		P(A)  = 8/20 = 2/5
		P(B)  = 1 / 2
					0.7 * 0.5
		P(B|A)= ----------------- = 7/8
						0.4

TF-IDF
-----------------
	1.TF
		term frequence,词频
		某个单词在文档中出现的频率。
		词频和文章的内容或者主题相关。

						3(hello)
		TF(hello) = --------------- = 0.03
						100(单词)
	2.IDF
		inverse docuement frequence,逆文档频率。
		针对某个单词进行衡量的。 
		特定单词对文档集区分的价值度。
		值越小，价值越低，说明出现该单词的文档越多。

						|D|
		idf(x) = log ---------
						P(x)
							
							1000
		idf(hello) = log -------------- = 0
						  x + 1 | (1000)
	
	3.TF-IDF
		a)hello 1000文章  1000出现hello 

			TF    * IDF    = 
			3
			---   * 0 = 0
			100     
		
		b)hello 1000文章 1出现
			TF    * IDF    = 
			3
			---   * 3 = 0.09
			100

			0 * 3 = 0 

最小二乘法
---------------
	最小平方法。
	(x1 - xn)^2 + (x2 - xn)^2 + ...
	-------------------------------
				n


逻辑回归
---------------
	
线性回归
-------------
	
逻辑回归
-------------
	

酒质量预测
---------------------
	1.引入maven依赖
        <dependency>
            <groupId>org.apache.spark</groupId>
            <artifactId>spark-mllib_2.11</artifactId>
            <version>2.1.0</version>
        </dependency>
	2.编程
		/**
		  * 使用spark的线性回归预测红酒质量
		  */

		import org.apache.spark.{SparkConf, SparkContext}
		import org.apache.spark.ml.regression.LinearRegression
		import org.apache.spark.ml.param.ParamMap
		import org.apache.spark.ml.linalg.{Vector, Vectors}
		import org.apache.spark.sql.{Row, SparkSession}

		object SparkMLLibLinearRegress {


			def main(args: Array[String]): Unit = {

				val conf = new SparkConf()
				conf.setAppName("ml_linearRegress")
				conf.setMaster("local[*]")

				val spark = SparkSession.builder().config(conf).getOrCreate()
				//1.定义样例类
				case class Wine(FixedAcidity: Double, VolatileAcidity: Double, CitricAcid: Double,
								ResidualSugar: Double, Chlorides: Double, FreeSulfurDioxide: Double,
								TotalSulfurDioxide: Double, Density: Double, PH: Double,
								Sulphates: Double, Alcohol: Double,
								Quality: Double)

				//2.加载csv红酒文件，变换形成rdd
				val file = "file:///D:\\ml\\data\\red.csv" ;
				val wineDataRDD = spark.sparkContext.textFile(file)
					.map(_.split(";"))
					.map(w => Wine(w(0).toDouble, w(1).toDouble, w(2).toDouble, w(3).toDouble,
						w(4).toDouble, w(5).toDouble, w(6).toDouble, w(7).toDouble, w(8).toDouble,
						w(9).toDouble, w(10).toDouble,
						w(11).toDouble))

				//导入sparksession的隐式转换对象的所有成员，才能将rdd转换成Dataframe
				import spark.implicits._
				val trainingDF = wineDataRDD.map(w => (w.Quality, Vectors.dense(w.FixedAcidity, w.VolatileAcidity,
					w.CitricAcid, w.ResidualSugar, w.Chlorides, w.FreeSulfurDioxide, w.TotalSulfurDioxide,
					w.Density, w.PH, w.Sulphates, w.Alcohol))).toDF("label", "features")
				trainingDF.show(100,false)

				//3.创建线性回归对象
				val lr = new LinearRegression()

				//4.设置回归对象参数
				lr.setMaxIter(2)

				//5.拟合模型
				val model = lr.fit(trainingDF)

				//6.构造测试数据集
				val testDF = spark.createDataFrame(Seq((5.0, Vectors.dense(7.4, 0.7, 0.0, 1.9, 0.076, 25.0, 67.0, 0.9968, 3.2, 0.68, 9.8)),
					(5.0, Vectors.dense(7.8, 0.88, 0.0, 2.6, 0.098, 11.0, 34.0, 0.9978, 3.51, 0.56, 9.4)),
					(7.0, Vectors.dense(7.3, 0.65, 0.0, 1.2, 0.065, 15.0, 18.0, 0.9968, 3.36, 0.57, 9.5))))
					.toDF("label", "features")

				//7.对测试数据集注册临时表
				testDF.createOrReplaceTempView("test")

				//8.使用训练的模型对测试数据进行预测,并提取查看项
				val tested = model.transform(testDF)
				tested.show(100,false)

				//
				val tested2 = tested.select("features", "label", "prediction")
				tested2.show(100,false)

				//9.展示预测结果
				tested.show()

				//10.通过测试数据集只抽取features，作为预测数据。
				val predictDF = spark.sql("select features from test")
				predictDF.show(100,false)
				model.transform(predictDF).show(1000,false)
			}
		}

模型持久化
-------------
	1.保存模型
		model.save("file:///d:/java/ml/lrmodel");
		
	2.加载模型
		LinearRegressModel.load("file:///d:/java/ml/lrmodel")


分类学习
--------------
	1.准备白酒数据
		white.csv
	2.编程
		/**
		  * 使用spark的逻辑回归分类白酒质量
		  */

		import org.apache.spark.SparkConf
		import org.apache.spark.ml.classification.LogisticRegression
		import org.apache.spark.ml.linalg.Vectors
		import org.apache.spark.ml.regression.LinearRegressionModel
		import org.apache.spark.sql.SparkSession

		object SparkMLLibLogisticRegress {


			def main(args: Array[String]): Unit = {

				val conf = new SparkConf()
				conf.setAppName("ml_linearRegress")
				conf.setMaster("local[*]")

				val spark = SparkSession.builder().config(conf).getOrCreate()
				//1.定义样例类
				case class Wine(FixedAcidity: Double, VolatileAcidity: Double,
								CitricAcid: Double, ResidualSugar: Double, Chlorides: Double,
								FreeSulfurDioxide: Double, TotalSulfurDioxide: Double, Density: Double,
								PH:Double, Sulphates: Double, Alcohol: Double, Quality: Double)

				//2.加载csv红酒文件，变换形成rdd
				val file = "file:///D:\\ml\\data\\white.csv" ;
				val wineDataRDD = spark.sparkContext.textFile(file)
					.map(_.split(";"))
					.map(w => Wine(w(0).toDouble, w(1).toDouble, w(2).toDouble, w(3).toDouble,
						w(4).toDouble, w(5).toDouble, w(6).toDouble, w(7).toDouble, w(8).toDouble,
						w(9).toDouble, w(10).toDouble,
						w(11).toDouble))

				//导入sparksession的隐式转换对象的所有成员，才能将rdd转换成Dataframe
				import spark.implicits._
				val trainingDF = wineDataRDD.map(w => (if (w.Quality < 7) 0D else
				1D, Vectors.dense(w.FixedAcidity, w.VolatileAcidity, w.CitricAcid,
				w.ResidualSugar, w.Chlorides, w.FreeSulfurDioxide, w.TotalSulfurDioxide,
				w.Density, w.PH, w.Sulphates, w.Alcohol))).toDF("label", "features")

				//3.创建逻辑回归对象
				val lr = new LogisticRegression()
				lr.setMaxIter(10).setRegParam(0.01)

				//4.拟合训练数据，生成模型
				val model = lr.fit(trainingDF)

				//5.构造测试数据
				val testDF = spark.createDataFrame(Seq(
					(1.0,Vectors.dense(6.1, 0.32, 0.24, 1.5, 0.036, 43, 140, 0.9894, 3.36, 0.64, 10.7)),
					(0.0,Vectors.dense(5.2, 0.44, 0.04, 1.4, 0.036, 38, 124, 0.9898, 3.29, 0.42, 12.4)),
					(0.0,Vectors.dense(7.2, 0.32, 0.47, 5.1, 0.044, 19, 65, 0.9951, 3.38, 0.36, 9)),
					(0.0,Vectors.dense(6.4, 0.595, 0.14, 5.2, 0.058, 15, 97, 0.991, 3.03, 0.41, 12.6)))
				).toDF("label", "features")

				testDF.createOrReplaceTempView("test")

				//预测测试数据
				val tested = model.transform(testDF).select("features", "label", "prediction")

				//
				val realData = spark.sql("select features from test")

				model.transform(realData).select("features","prediction").show(100,false)
			}
		}



垃圾邮件分类
-----------------
	1.代码
		/**
		  * 使用spark的逻辑回归管线实现垃圾邮件分类
		  */
		import org.apache.spark.SparkConf
		import org.apache.spark.ml.classification.LogisticRegression
		import org.apache.spark.ml.param.ParamMap
		import org.apache.spark.ml.linalg.{Vector, Vectors}
		import org.apache.spark.sql.{Row, SparkSession}
		import org.apache.spark.ml.Pipeline
		import org.apache.spark.ml.feature.{HashingTF, RegexTokenizer, StopWordsRemover, Tokenizer, Word2Vec}

		object SparkMLLibSpamFilterRegress {


			def main(args: Array[String]): Unit = {

				val conf = new SparkConf()
				conf.setAppName("ml_linearRegress")
				conf.setMaster("local[*]")

				val spark = SparkSession.builder().config(conf).getOrCreate()

				//1.创建训练数据集
				val training = spark.createDataFrame(Seq(
					("you@example.com", "hope you are well", 0.0),
					("raj@example.com", "nice to hear from you", 0.0),
					("thomas@example.com", "happy holidays", 0.0),
					("mark@example.com", "see you tomorrow", 0.0),
					("xyz@example.com", "save money", 1.0),
					("top10@example.com", "low interest rate", 1.0),
					("marketing@example.com", "cheap loan", 1.0)))
					.toDF("email", "message", "label")

				//2.创建分词器(压扁行成单词集合)
				val tokenizer = new Tokenizer().setInputCol("message").setOutputCol("words")

				//3.将单词换算成特征向量()
				val hashingTF = new HashingTF().setNumFeatures(1000).setInputCol("words").setOutputCol("features")

				//4.逻辑回归
				val lr = new LogisticRegression().setMaxIter(10).setRegParam(0.01)

				//将2,3,4合成一个链条
				val pipeline = new Pipeline().setStages(Array(tokenizer, hashingTF, lr))

				//通过管线对训练数据进行拟合，生成管线模型。
				val model = pipeline.fit(training)

				//准备测试数据
				val test = spark.createDataFrame(Seq(
					("you@example.com", "how are you"),
					("jain@example.com", "hope doing well"),
					("caren@example.com", "want some money"),
					("zhou@example.com", "secure loan"),
					("ted@example.com", "need loan")))
					.toDF("email", "message")

				//对测试数据进行预测
				val prediction = model.transform(test).select("email", "message", "prediction").show(1000,false)


			}
		}

南京市长江大桥

中文分词
----------------
	1.引入依赖
        <dependency>
            <groupId>org.apdplat</groupId>
            <artifactId>word</artifactId>
            <version>1.3</version>
        </dependency>

	2.代码
		import org.apdplat.word.WordSegmenter;
		import org.apdplat.word.dictionary.DictionaryFactory;
		import org.apdplat.word.segmentation.Word;
		import org.apdplat.word.util.WordConfTools;

		import java.util.List;

		/**
		 * 中文分词
		 */
		public class ZhTokonizer {
			public static void main(String[] args) {
				//WordConfTools.set("dic.path", "classpath:dic.txt，d:/custom_dic");
				WordConfTools.set("stopwords.path", "classpath:mystops.txt");
				DictionaryFactory.reload();//更改词典路径之后，重新加载词典

				List<Word> list = WordSegmenter.seg("南京市的长江大桥是最长的大桥");
				for(Word w: list){
					System.out.println(w.getText());
				}
			}
		}
	
	3.自定义停用词
		mystops.txt
		南京
		南京市

Tokenizer
--------------------
	对训练数据进行变换，将指定列计算后，生成新的列.
	本质上就是对一行文本进行压扁.
	val tokenizer = new Tokenizer().setInputCol("message").setOutputCol("words")
	val df = tokenizer.transform(training)
	df.show(100,false)

HashingTF
--------------------
	将单词映射到指定维度的数组中。
	val hashingTF = new HashingTF().setNumFeatures(1000).setInputCol("words").setOutputCol("features")
	val df2 = hashingTF.transform(df) ;
	df2.show(1000,false)


密度向量
---------------
	Vectors.dense()
	每个向量的值都存储.

松散向量
---------------
	Vectors.sparse()
	只存储非0值。

	1000,([8,90,200],[2,40,80])




